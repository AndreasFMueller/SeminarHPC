\chapter{(S) SOR\label{chapter:sor}}
\lhead{(S) SOR}
\begin{refsection}
\chapterauthor{Felix Hofer}


\section{Einf"uhrung}
\rhead{Einf"uhrung}
``Successive Overrelaxation''
\cite{Watkins:FundamentalsOfMatrixComputations} -- kurz SOR -- bezeichnet eine
\index{SOR}
\index{Overrelaxation}
\index{Successive Overrelaxation}
Modifikation bestehender iterativer Approximationsverfahren, welche
die Konvergenzgeschwindigkeit gegen"uber den unmodifizierten Verfahren
erh"ohen soll. Das Prinzip wird oft anhand des Gauss-Seidel-Algorithmus
illustriert, kann aber genauso gut auch auf andere iterative
Approximationsverfahren zur L"osung linearer Gleichungssystemen angewendet
werden.
\index{Relaxationsparameter}

Die grundlegende Idee hinter dem SOR-Prinzip ist, dass eine einmal
eingeschlagene Richtung hin zur schlussendlichen L"osung so falsch
nicht sein kann, weshalb man gleich einen etwas gr"osseren Schritt
in die entsprechende Richtung wagt (daher auch die Bezeichnung
\textbf{over}relaxation) \cite{SOR_Dorn}. Zus"atzlich l"asst man den
vorhergehenden
Iterationsschritt in die Berechnung der neuen Approximation der echten
L"osung mit einfliessen.

Wie stark man den neuen Schritt ``"uberrelaxiert'' bzw.~wie stark der
vorhergehende Iterationsschritt noch in die Berechnung mit einfliesst,
steuert man mittels des sogenannten ("Uber-) Relaxationsparameters
$\omega$, dessen Wahl einen entscheidenden Einfluss auf die
Konvergenzgeschwindigkeit
des SOR-Verfahrens hat (siehe Abschnitt~\ref{subsec: Konv.geschw.analyse}).
Mit ihm kann sie gegen"uber dem unrelaxierten Algorithmus enorm gesteigert,
bei Wahl eines suboptimalen $\omega$ aber auch verringert werden. Bei Wahl
eines falschen $\omega$ divergiert das SOR-Verfahren (siehe
Abschnitt~\ref{sec: Konvergenz}).
Dabei unterscheidet man drei F"alle:
\begin{itemize}
\item $\boldsymbol{\omega > 1}$\textbf{:} Im Normalfall wird man einen
"Uberrelaxationsparameter w"ahlen, um die Schrittweite gegen"uber dem
unrelaxierten Algorithmus zu vergr"ossern und so die Konvergenz zu
beschleunigen.
\item $\boldsymbol{\omega = 1}$\textbf{:} Setzt man f"ur $\omega$ genau 1 ein,
so ergibt sich die urspr"ungliche Variante des unmodifizierten Algorithmus
(vergleiche dazu die Gleichungen \ref{eq: G.-S.-It.formel} und \ref{eq:
SOR-It.formel1}).
\item $\boldsymbol{\omega < 1}$\textbf{:} Es ist denkbar, den
Relaxationsparameter $\omega$
kleiner als 1 zu w"ahlen. So kann man die Berechnung eines ansonsten leicht
divergierenden
Problems stabilisieren. Allerdings wird dadurch die
Konvergenzgeschwindigkeit verringert,
da man auf diese Weise einen kleineren Schritt in Richtung echte L"osung
macht als mit dem
unmodifizierten Algorithmus.
Daf"ur fliesst der letzte Iterationsschritt mehr in die Berechnung mit ein.
\end{itemize}

\section{SOR-Algorithmus\label{sec: SOR-Alg.}}
\rhead{SOR-Algorithmus}
Zur Illustration des SOR-Algorithmus und zur Herleitung einer
Iterationsformel nehmen wir die unmodifizierte Variante von
Gauss-Seidel (siehe Kapitel \ref{section-gauss-seidel},
Gleichung~\ref{gauss-seidel-iteration}) als Vorlage, die den unrelaxierten
Iterationsschritt eines einzelnen $x_i$ repr"asentiert:
\begin{equation} \label{eq: G.-S.-It.formel}
x_i^{(k+1)} = \frac{1}{a_{ii}}\left(b_i-\sum\limits_{j=1}^{i-1} a_{ij}x_j^{(k+1)}-\sum\limits_{j=i+1}^na_{ij}x_j^{(k)}\right)
\end{equation}
Dieser Iterationsschritt wird nun mit $\omega$ relaxiert -- das heisst
gestreckt. Zus"atzlich l"asst man, abh"angig von $\omega$, das $x_i$
des letzten Iterationsschrittes in die Berechnung des neuen $x_i$ mit
einfliessen, was zur folgenden, f"ur SOR modifizierten Iterationsformel
von Gauss-Seidel f"uhrt:
\begin{equation} \label{eq: SOR-It.formel1}
	x_i^{(k+1)} = \frac{\boldsymbol{\textcolor{red}{\omega}}}{a_{ii}}\left(b_i-\sum\limits_{j=1}^{i-1}a_{ij}x_j^{(k+1)}-\sum\limits_{j=i+1}^n a_{ij}x_j^{(k)}\right)\boldsymbol{\textcolor{red}{+(1-\omega)x_i^{(k)}}}
\end{equation}
Auf die Wahl des optimalen Wertes f"ur $\omega$ wird sp"ater eingegangen.

\subsection{Herleitung von Matrix-Zerlegung und Iterationsformel \label{subsec: SOR-Herleitung}}
Wie Jacobi und Gauss-Seidel basiert auch der SOR-Algorithmus auf einer
Matrix-Aufteilung in ``einfach'' invertierbare Matrizen wie zum Beispiel
Dreiecks- oder Diagonalmatrizen. Um auf die schlussendliche Matrix-Formel
zu kommen,
wird obige Summenformel (\ref{eq: SOR-It.formel1}) nach $x_i^{(k)}$
und $x_i^{(k+1)}$
getrennt und anschliessend mit $a_{ii}$ multipliziert, damit in der
endg"ultigen Iterationsformel dann nicht $D^{-1}$ vorkommt:
\begin{equation} \label{eq: SOR-It.formel2}
	%for\ i=1, ..., n\\
	%\left[
	%\underbrace{
	a_{ii}x_i^{(k+1)} %}_{\substack{Dx^{(k+1)}}}
	+\omega %\underbrace{
	\sum\limits_{j=1}^{i-1} a_{ij}x_j^{(k+1)} %}_{\substack{Lx^{(k+1)}}}
 = \omega b_i- \omega %\underbrace{
 \sum\limits_{j=i+1}^n a_{ij}x_j^{(k)} %}_{\substack{Ux^{(k)}}}
 +(1-\omega)	%\underbrace{
	a_{ii}x_i^{(k)} %}_{\substack{Dx^{(k)}}}
	%\right.
\end{equation}
Wenn man nun mit Gleichung (\ref{eq: SOR-It.formel2}) einen kompletten
Iterationsschritt f"ur s"amtliche Vektor-Kompo\-nenten $x_i$ mit
$i=1,\dots,n$ ausf"uhrt, so kann man einzelne Teile der Summenformel
durch Teilmengen der Ausgangsmatrix A audr"ucken und diese so als
vereinfachte Matrix-Gleichung \cite{wikiEn_SOR} umschreiben (vergleiche
dazu auch
Kapitel~\ref{section-matrix-aufteilung},
Gleichung~\ref{gauss-seidel:herleitung}):
\begin{equation} \label{eq: SOR-It.formel3}
Dx^{(k+1)}+\omega Lx^{(k+1)}=\omega b-\omega Ux^{(k)}+(1-\omega)\cdot Dx^{(k)}
\end{equation}
Wobei $D$ der Diagonale von $A$ entspricht, $L$ dem Teil unterhalb
der Diagonale und $U$ dem Teil oberhalb. Somit ergibt sich f"ur die
Ausgangsmatrix A mit SOR folgende Zerlegung, in der zum Beispiel auch
das Jacobi- oder das Gauss-Seidel-Verfahren dargestellt werden k"onnen:
\begin{equation} \label{eq: SOR Matrix-Splitting}
A=D+L+U
\end{equation}
Klammert man in Gleichung (\ref{eq: SOR-It.formel3}) nun noch $x^{(k)}$
und $x^{(k+1)}$ aus und multipliziert diese dann mit $(D+\omega
L)^{-1}$, so erh"alt man die endg"ultige SOR-Iterationsformel f"ur den
Gauss-Seidel-Algorithmus als Matrix-Gleichung:
\begin{equation} \label{eq: SOR-It.formel}
x^{(k+1)}=\underbrace{(D+\omega L)^{-1}}_{\substack{M^{-1}}}\cdot(\omega b-\underbrace{[\omega U+(\omega-1)\cdot D]}_{\substack{N}}\cdot x^{(k)}).
\end{equation}


\section{Wahl des optimalen Relaxationsparameters}
\rhead{Wahl des optimalen Relaxationsparameters}

Da $\omega$ von der jeweiligen Ausgangsmatrix $A$ abh"angig ist,
lassen sich leider keine allgemein g"ultigen Aussagen "uber die Wahl
des optimalen Wertes f"ur $\omega$ machen. In der Praxis werden zur Absch"atzung
des optimalen Wertes daher meist heuristische Verfahren angewendet.
F"ur verschiedene bekannte Probleme gibt es aber mittlerweile auch Formeln,
mit denen sich das optimale $\omega$ ann"ahernd oder auch exakt berechnen
l"asst.


F"ur einige bestimmte Matrix-Arten, wie zum Beispiel tridiagonale
Blockmatrizen, l"asst sich $\omega$ direkt berechnen. Oft ist die
Berechnung entsprechender Formeln oder aber auch nur schon Teilen daraus
jedoch so aufw"andig, dass auch hier anfangs wieder auf heuristische oder aber
wie in Abschnitt~\ref{subsec:
Konv.geschw.analyse} auf empirische Verfahren zur"uckgegriffen werden muss.

\subsection{Analyse der Konvergenzgeschwindigkeit \label{subsec: Konv.geschw.analyse}}
Zur Analyse der Konvergenzgeschwindigkeit der verschiedenen
Verfahren und $\omega$ wurde das Zahlenbeispiel aus
Kapitel~\ref{section-matrix-aufteilung} verwendet.
Der Spektralradius $\rho(G)$ wurde mit Matlab als
\texttt{max(abs(eig(G)))} berechnet und das optimale $\omega$ ebenfalls
mit Matlab mit einer for-Schleife in Hundertstel-Schritten
zwischen $\omega=1$ und $\omega=2$ gesucht.
Vergleicht man den Spektralradius der verschiedenen Verfahren und
$\omega$, kann man deutliche Unterschiede erkennen (siehe
Abbildungen~\ref{fig:Spekt.rad.vs.omega}
und \ref{fig:Spekt.rad. Diagr.}).
\begin{figure}\centering
\includegraphics[width=0.48\textwidth]{sor/img/Spektralradius.png}
\hfill
\includegraphics[width=0.48\textwidth]{sor/img/NumIt.png}
\caption{Spektralradius (links) und
notwendige Anzahl Iterationen f"ur  eine Genauigkeit von 4 Nachkommastellen
f"ur die L"osung (rechts)
des Problems in Abh"angigkeit von $\omega$.
\label{fig:Spekt.rad.vs.omega}
\label{fig:Spekt.rad.vs.Anz.It.}
}
\end{figure}

\begin{figure}\centering
\includegraphics[width=0.75\textwidth]{sor/img/Spektralradius_Diagramm_2.png}
\includegraphics[width=0.75\textwidth]{sor/img/NumIt_Diagramm_2.png}
\caption{Spektralradien (oben) und
Notwendige Anzahl Iterationen (unten) f"ur  eine Genauigkeit von
4 Nachkommastellen f"ur verschiedene Verfahren und $\omega$.
\label{fig:Spekt.rad. Diagr.}
\label{fig:It.vs.omega}
}
\end{figure}

Mit der Formel 
\begin{equation} \label{eq: Accuracy}
n=\frac{-k \log{(10)}}{\log{(\rho(G))}}
\end{equation}
wurde die notwendige Anzahl Iterationsschritte f"ur eine Genauigkeit von
$k=4$ Nachkommastellen berechnet und ebenfalls f"ur die verschiedenen
Verfahren und $\omega$ visualisiert. Wie nicht anders zu erwarten
sind auf den Abbildungen~\ref{fig:Spekt.rad.vs.Anz.It.} und
\ref{fig:It.vs.omega} auch hier deutliche Unterschiede zu erkennen.

\subsection{Visualisierung der Iterationsschritte}
Die Abbildung~\ref{fig:3D Bsp} zeigt die effektiv errechneten
Iterationsschritte mit dem Jacobi-, dem Gauss-Seidel- und dem in
Abschnitt~\ref{sec: SOR-Alg.} beschriebenen SOR-Verfahren in einem 2D-
bzw.~3D-Koordinaten\-system.
Anhand diesen kann man gut erkennen, wie man beim SOR-Verfahren mit
einem zu grossen $\omega$ "uber das eigentliche Ziel hinausschiesst,
bei Wahl eines guten $\omega$ mit gr"osseren Schritten gegen"uber Jacobi
und Gauss-Seidel aber auch schneller dort angelangt.
\begin{figure}\centering
\includegraphics[width=0.475\textwidth]{sor/img/Jac_2D_14It.png}\hfill
\includegraphics[width=0.475\textwidth]{sor/img/G_S_2D_7It.png}\\[1mm]
%\caption{Jacobi, 14 Iterationen (l.); Gauss-Seidel, 7 Iterationen (r.)}
%\label{fig:Jac./G-S 2D Bsp}
%\end{figure}
%\begin{figure}[h!]
\includegraphics[width=0.475\textwidth]{sor/img/SOR_2D_4_It_w_1_08.png}\hfill
\includegraphics[width=0.475\textwidth]{sor/img/SOR_2D_8_It_w_1_3.png}\\[1mm]
%\caption{SOR, 4 Iterationen, $\omega=1.08=\omega_{\text{opt}}$ (l.); SOR, 8 Iterationen, $\omega=1.3$ (r.)}
%\label{fig:SOR 2D Bsp1}
%\end{figure}
%\begin{figure}[h!]\centering
\includegraphics[width=0.475\textwidth]{sor/img/SOR_2D_19_It_w_1_6.png}
\caption{Von links nach rechts und oben nach unten: Jacobi, 14 Iterationen;
Gauss-Seidel, 7 Iterationen.; SOR, 4 Iterationen, $\omega=1.08=\omega_{\text{opt}}$;
SOR, 8 Iterationen, $\omega=1.3$; SOR, 19 Iterationen, $\omega=1.6$}
\label{fig:SOR 2D Bsp2}
\end{figure}
\begin{figure}\centering
\includegraphics[width=0.475\textwidth]{sor/img/Jacobi_3D_46_It.png}\hfill
\includegraphics[width=0.475\textwidth]{sor/img/G_S_3D_23_It.png}\\[1mm]
%\caption{Jacobi, 46 Iterationen; Gauss-Seidel, 23 Iterationen (r.)}
%\label{fig:Jac./G-S 3D Bsp}
%\end{figure}
%\begin{figure}[h!]
\includegraphics[width=0.475\textwidth]{sor/img/SOR_3D_18_It_w_1_1.png}\hfill
\includegraphics[width=0.475\textwidth]{sor/img/SOR_3D_8_It_w_1_27.png}\\[1mm]
\includegraphics[width=0.475\textwidth]{sor/img/SOR_3D_19_It_w_1_6.png}
\caption{Von links nach rechts und oben nach unten: Jacobi, 46 Iterationen;
Gauss-Seidel, 23 Iterationen; SOR, 18 Iterationen, $\omega=1.1$;
SOR, 8 Iterationen, $\omega=1.27=\omega_{\text{opt}}$;
SOR, 19 Iterationen, $\omega=1.6$}
\label{fig:3D Bsp}
\end{figure}


Wie bei den Abschnitt~\ref{fig:Spekt.rad.vs.Anz.It.} und
\ref{fig:It.vs.omega} zeigt auch die Abbildung~\ref{fig:3D Bsp}
die ben"otigten Schritte f"ur eine Genauigkeit von 4 Nachkommastellen.
Die 2D-Beispiele wurden anhand dem Zahlenbeispiel zum
Jacobi-Algorithmus aus Kapitel~\ref{section-jacobi} erstellt.
Die 3D-Beispiele wurden anhand dem Zahlenbeispiel zum
Gauss-Seidel-Algorithmus aus Kapitel \ref{section-matrix-aufteilung}
(bzw.~Abschnitt~\ref{subsec: Konv.geschw.analyse}) erstellt.



\section{SSOR-Prinzip}
\rhead{SSOR-Prinzip}
\index{SSOR}
\index{Symmetric SOR}
``Symmetric SOR'' ist eine Erweiterung des in Abschnitt~\ref{sec: SOR-Alg.}
beschriebenen SOR-Algorithmus. In einem einzelnen Iterationsschritt
folgt bei SSOR auf den in Gleichung (\ref{eq: SOR-It.formel1})
beschriebenen ``Vorw"arts-Schritt'' von SOR (mit $i=1,\dots,n$) nun
noch ein zus"atzlicher Schritt, bei dem die Vektor-Kompo\-nenten $x_i$ in
umgekehrter Reihenfolge (also mit $i=n,\dots, 1$) berechnet werden. Dies
f"uhrt somit zu dem folgenden, vollst"andigen SSOR-Iterationsschritt
f"ur den Gauss-Seidel-Algorithmus (vergleiche dazu Abschnitt~\ref{subsec:
SOR-Herleitung}, Gleichung~\ref{eq: SOR-It.formel}):
\begin{equation} \label{eq: SSOR-It.formel-k+1/2}
	x^{(k+\frac{1}{2})}=(D+\omega L)^{-1}\cdot(\omega b-[\omega U+(\omega-1)\cdot D]\cdot x^{(k)})
\end{equation}
\begin{equation} \label{eq: SSOR-It.formel-k+1}
	x^{(k+1)}=(D+\omega U)^{-1}\cdot(\omega b-[\omega L+(\omega-1)\cdot D]\cdot x^{(k+\frac{1}{2})})
\end{equation}
Dabei nutzt man aus, dass der Gauss-Seidel-Algorithmus -- im Gegensatz
zum simultanen Jacobi-Algorithmus -- sukzessive ist. Das heisst, die
Resultate der einzelnen $x_i$ sind abh"angig von der Reihenfolge derer
Berechnung, da bei Gauss-Seidel jedes $x_i$ mit $i>1$ (bzw.~in Gleichung
\ref{eq: SSOR-It.formel-k+1} mit $i<n$) abh"angig von im aktuellen
Iterationsschritt bereits berechneten $x_i$ ist. Deshalb auch der Name
``\textbf{Successive} Overrelaxation''.


W"urde man das SSOR-Prinzip auf den Jacobi-Algorithmus anwenden, so k"ame
dies zwei gew"ohnlichen SOR-Iterationsschritten gleich, da bei Jacobi
noch zu berechnende $x_i$ unabh"angig von im aktuellen Iterationsschritt
bereits berechneten $x_i$ sind.

Es soll an dieser Stelle erw"ahnt sein, dass SSOR "ublicherweise keine
Effizienzsteigerung gegen"uber SOR bietet. Die Konvergenz ist meist sogar
schlechter als mit SOR. Der Einsatz des SSOR-Verfahrens dient deshalb
eher der Vorkonditionierung \cite{Vorkonditionierung_Dorn} eines Problems
(sogenannter ``Preconditioner''). Dies bezeichnet die Umwandlung des
eigentlichen Ausgangsproblems in ein effizienter l"osbares Problem, zum
Beispiel f"ur nicht station"are iterative Verfahren. Die Ausgangsmatrix
soll auf diese Weise dahingehend vorbereitet werden, dass sich in der
weiteren Verarbeitung durch Vereinfachungsmethoden f"ur schwach besetzte
Matrizen m"oglichst Vorteile ergeben (verbesserte Spektraleigenschaften),
nicht jedoch die eigentliche L"osung des Systems beeinflusst wird.
\index{Preconditioner}
\index{Vorkonditionierung}

\section{Konvergenz} \label{sec: Konvergenz}
\rhead{Konvergenz}
\index{Konvergenz}
Im Jahre 1947 hat der russisch-deutsch-schweizerische Mathematiker
Alexander Markowitsch Ostrowski \cite{AlexanderMarkowitschOstrowski}
\index{Ostrovski, Alexander Markowitsch}
bewiesen, dass der SOR-Algorithmus f"ur positiv definitive
Ausgangsmatrizen A konvergiert, wenn $\omega \in (0,2)$. In diesem Falle
ist also der Spektralradius $\rho(G) < 1$ mit
\begin{equation} \label{Spektralradius}
G=M^{-1}N,\hspace{.1\textwidth}
M=D+\omega L,\hspace{.1\textwidth}
N=\omega U+(\omega-1)\cdot D.
\end{equation}
Im Jahre 1958 hat ausserdem der kanadische Mathematiker und Informatiker
William Kahan \cite{WilliamKahan} bewiesen, dass f"ur den SOR-Algorithmus
\index{Kahan, William}
mit $\omega < 0$ oder $\omega > 2$ keine Konvergenz mehr vorliegt.

\nocite{Watkins:FundamentalsOfMatrixComputations, SOR_Dorn, Vorkonditionierung_Dorn,wikiEn_SOR, wikiDe_SOR, SSOR, AlexanderMarkowitschOstrowski, WilliamKahan}		
\printbibliography[heading=subbibliography]
\end{refsection}
